### Named Entity Recognition (NER) over general purpose text domain ###

This project implements a Named Entity Recognition (NER) system designed to identify and classify entities (such as Names, Organizations, and Locations) within unstructured text. This was developed as part of the NLP course at University of Padua (UNIPD).
In particular, we leveraged Gemini API to analyze different Named Entity Recognition techniques for general purpose Few-NERD Dataset.

## Project Overview ##

**Token Level Evaluation**
| Technique | Accuracy | Precision | Recall | F1 Score |
| :---: | :---: | :---: | :---: | :---: |
| **Vanilla** | 0.8961 | 0.9005 | 0.8961 | 0.8959 |
| **Vanilla + Online Search** | 0.8616 | 0.9075 | 0.8616 | 0.8750 |
| **Vanilla + Chain of Thought** | 0.9009 | 0.9070 | 0.9009 | 0.9013 |
| **Vanilla + Iterative Refinement** | 0.8925 | 0.8948 | 0.8925 | 0.8917 |
| **Few-shot** | 0.9049 | 0.8940 | 0.9049 | 0.8970 |

**Boundary Level Evaluation**
| Technique | Accuracy | Precision | Recall | F1 Score |
| :--- | :---: | :---: | :---: | :---: |
| **Vanilla** | 0.4854 | 0.4372 | 0.4854 | 0.4543 |
| **Vanilla + Online Search** | 0.4173 | 0.3656 | 0.4173 | 0.3748 |
| **Vanilla + Chain of Thought** | 0.4496 | 0.4231 | 0.4496 | 0.4273 |
| **Vanilla + Iterative Refinement** | 0.4556 | 0.3956 | 0.4556 | 0.4192 |
| **Few-shot** | 0.5033 | 0.4998 | 0.5033 | 0.4978 |

The following analysis is simply based on 1 test run, therefore the next considerations are just hypotheses:

- **Performance gap**: token-level evaluation shows consistently high performance (89-90%) while boundary-level evaluation is much lower (37-50%), possibly indicating that the model struggles with precise entity boundary detection.
- **Few-shot learning**: few-shot learning achieves the best boundary-level performance (F1: 0.4978) and second-best token-level performance; perhaps making it the most balanced and practical approach for real-world NER applications.
- **Online Search considerations**: adding online search actually degrades performance across both evaluation levels, suggesting that external information may introduce noise rather than helpful context for this NER task.

# Issues encountered #

- For metrics computation, sequence alignment between the input sentence tokens and the entities generated by the LLM
- LLM had a hard time detecting the correct span boundary
- Daily API requests limit
- Accurate reconstruction of the sentence from the tokens
More details can be found inside the notebook.

## Python libraries ##

The following libraries were used for the project:

- **datasets**: Download Few-NERD dataset
- **nltk**: Used for computing pos-tags
- **matplotlib**: Needed for plotting dataset metrics
- **google-genai**: To configure Gemini client and send requests
- **python-dotenv**: Used to read enviroment files

## Gemini API usage ##

In order to use the Gemini API service, it is necessary to have a personal key. Follow this steps in order to create one:

- Go to https://ai.google.dev/gemini-api/docs/api-key
- Click on the blue button "Get a Gemini API key in Google AI Studio"
- On the top-right click on "Create API key"
- Search for "Gemini API" and create
- Copy your API key and paste it in the variable your_api_key_here (IMPORTANT)
